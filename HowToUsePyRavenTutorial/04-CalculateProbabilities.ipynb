{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate probabilites from the chi2 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we explain how to use a wrapper function to make all of the probability calculations necessary for a given star. We also show a wrapper function that will generate a PDF with useful diagnostic graphs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyRaven as rav\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. The calculation itself"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The commands are relatively simple. \n",
    "\n",
    "1. We load up the param object that was used to calculate the chi2 values.\n",
    "2. We use `create_lnLH_odds_from_chi` and `create_lnlH_pars_from_chi` to calculate the likelihoods from the chi2. NOTE: for the parameter estimation, param['grid']['noise_grid'] must be defined in the param structure. We recommend a grid between 0.1 and 2.0 with 0.1 intervals. \n",
    "3. We use the `combine_obs` wrapper to get all of the posterior probabilities. \n",
    "4. We use the `overview_plots` to create a PDF that contains a summary of all of the relevant information. \n",
    "\n",
    "> TODO: Maybe wrap the LH calculation inside of the combine_obs wrapper, and pass it the chi folder, the datapacket and the param?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The code below will crash, because the github does not have the chi2 data in the LoopExample folder (this is why it is in markdown, instead of a code cell). \n",
    ">\n",
    "> But if someone is following these tutorials, they will have the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````python\n",
    "# Read in the param structure that was used to create the chi square data\n",
    "# and the datapacket that was used. \n",
    "param = rav.params.read_parameters('ExampleData/LoopExample/param.json')\n",
    "datapacket = rav.data.read_packet('ExampleData/LoopExample/ExamplePacket.h5')\n",
    "\n",
    "# Calculate the likelihoof for the odds ratio calculations\n",
    "rav.BayesObjects.create_lnLH_odds_from_chi('ExampleData/LoopExample', param, datapacket)\n",
    "\n",
    "# Calculate the likelihood for the parameter estimation\n",
    "# For this you will need a grid for the noise scale parameter\n",
    "param['grid']['noise_grid'] = np.arange(0.1,2.1,0.1)\n",
    "rav.BayesObjects.create_lnLH_pars_from_chi('ExampleData/LoopExample', param, datapacket)\n",
    "\n",
    "# Calculate all of the posterior probabilities\n",
    "rav.BayesObjects.combine_obs(datapacket.nobs)\n",
    "\n",
    "# Make some diagnostic graphs\n",
    "rav.BayesObjects.overview_plots(datapacket.nobs)\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this wrapper is to make calculations and create associated h5 files with the output data. \n",
    "\n",
    "If needed, all of these files can be examined afterward by loading them up with the appropriate object class in pyRaven.BayesObject, which also provide class functions to manipulate (and graph) this data, if some custom analysis is needed. \n",
    "See LINK TO NOTEBOOK for details. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. So, what are all of those files?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To explain the outputs of the wrapper function, we need to quickly recap how the bayesian analysis works:\n",
    "\n",
    "We have two competitive hypothesis: The star is magnetic, the star is not magnetic.\n",
    "\n",
    "We have a model for the \"the star is magnetic\", which has some parameters $\\vec{\\theta}$ (Bpole, beta, incl, phi). The 'model' for the 'star is not magnetic\" is simply that Stokes V = 0, which has no model parameters. \n",
    "\n",
    "A few Bayesian definitions:\n",
    "\n",
    "* A hypothesis comparison: we compute the 'odds ratio' between two competing hypothesis (the star is magnetic versus the star is not magnetic)\n",
    "* A parameter estimation: we compute the probability of the model parameters assuming that the associated hypothesis is true. \n",
    "\n",
    "A few more definitions:\n",
    "\n",
    "* The prior is the probabilily of a given hypothesis/parameter in the absence of new data. \n",
    "* The likelihood is the probability of getting the current dataset if the hypothesis/parameters are the true hypothesis/parameters. This is closely related to (and calculated from) the chi square. \n",
    "* The posterior is the probability of a hypothesis/parameter while taking into account both the prior and the likelihood. \n",
    "\n",
    "And a few more definitions:\n",
    "\n",
    "* The output of a bayesian parameter estimation is not a probability but a probability density. Therefore to know the probability that the real value of parameter x is between x1 and x2, one must integrate the probability density between x1 and x2. \n",
    "\n",
    "* Normalization: the probability of a model over the whole set of parameters considered must be 1.0 (if the hypothesis is true, and the model is right, then the probability that the real value of the parameters is in the range covered by the parameters must be 1.0)\n",
    "* Marginalization: Imagine that we have a model with two parameters $\\theta_1$ and $\\theta_2$. The bayesian calculation gives us $P(\\theta_1, \\theta_2)$. But let's say that we are interested only in the probability density with respect to $\\theta_1$. In order word, what is the probability density for a particular value of $\\theta_1$ with $\\theta_2$ being able to be anything. Mathematically, this will be $P(\\theta_1) = \\int P(\\theta_1, \\theta_2)d\\theta_2$. \n",
    "* Noise scale parameter: when doing parameter estimation, it's a good idea to account for the possibility that our estimation for the error in our data might have been overestimated or underestimate (the latter could also be due to features in our data that the model cannot reproduce). We add this parameter as a scaling to the datapoint uncertainty. See LINK TO NOTEBOOK for details. \n",
    "\n",
    "More specifically about our bayesian model in the context of a magnetic star:\n",
    "\n",
    "* For multiple observations of a given star, the Bpole, incl and beta should remain the same. We are therefore interested in the probability density of each of these parameters when taking in account all of the observations together. However, the rotational phase (and the noise scale parameter) can change from observation to observation. Furthermore, in most applications of pyRaven, the rotational period is not known, therefore the rotational phase cannot be determined from an already known ephemeris. \n",
    "\n",
    "* Therefore the strategy is to caluculate the probability for (Bpole, beta, incl, phi) for each observations, marginalize the probability over the rotational phases (and noise scale parameter), and then combine the probabilities from each observations together. \n",
    "\n",
    "ASK TALI TO MAKE A FLOWCHART FOR THIS?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now let's talk about the files. In the list below, `[S]` stands for the Stokes parameter involved (\"V\" or \"N1\") and `[o]` stands for the observation index.\n",
    "\n",
    "- **`lnLH_PARS_[S]_obs[o].h5`:** Contains the (beta, Bpole, phi, incl, noise) likelihood for parameter estimation. (Read in with `read_lnP_pars()`)\n",
    "\n",
    "- **`lnpost_PARS_noprior_[S]_obs[o].h5`:** Contains the (beta, Bpole, phi, incl, noise) posterior probability calculated without taking priors into account (or in other words, using a flat prior for each parameter). This is can also be thought of as the equivalent of the normalized likelihood. (Read in with `read_lnP_pars()`)\n",
    "\n",
    "- **`lnpost_PARS_prior_[S]_obs[o].h5`:** Contains the (beta, Bpole, phi, incl, noise) normalized posterior probability for observation 'o' (with priors) (Read in with `read_lnP_pars()`)\n",
    "\n",
    "- **`lnpost_PARS_mar_noprior_[S]_obs[o].h5`:** Contains the (beta, Bpole, incl) posterior probability marginalzed for phi and noise, for observation 'o', with flat priors (Read in with `read_lnP_mar()`)\n",
    "\n",
    "- **`lnpost_PARS_mar_prior_[S]_obs[o].h5`:** Contains the (beta, Bpole, incl) posterior probability marginalzed for phi and noise, for observation 'o', with priors (Read in with `read_lnP_mar()`)\n",
    "\n",
    "- **`lnpost_PARS_mar_noprior_[S].h5`:** Contains the (beta, Bpole, incl) posterior probability for the combined observations, but for which the priors have been ignored (or in other words for which all priors were flat) (Read in with `read_lnP_mar()`)\n",
    "\n",
    "- **`lnpost_PARS_mar_prior_[S].h5`:** Contains the (beta, Bpole, incl) posterior probability for the combined observations (Read in with `read_lnP_mar()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The plot_summary function creates a PDF file with a variety of diagnostic graphs. \n",
    "\n",
    "The first series of graphs are the marginalized probability density for the (Bpole, beta, phi, incl, noise) parameters for each observations. \n",
    "* Dashed curve is the posterior without prior (i.e. flat priors, also can be thought of as the normalized likelihood)\n",
    "* Solid curve is the posterior with the priors\n",
    "* Pink curve is the 1D prior for each parameter. \n",
    "\n",
    "The second series of graphs show the corner plot for each observation with the phi and noise marginalized (for each observation, therefore the combination has not taken place yet). Stokes V is the on left and the null 1 profile is on the right. For each observations there are two consecutive pages:\n",
    "* First page is with flat priors\n",
    "* Second page is with the priors. \n",
    "\n",
    "Finally at the end, the last two corner plots are for the combined observations. Again, there are two pages:\n",
    "* First page is with flat priors\n",
    "* Second page is with the priors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Still TODO: Make the posterior calculation for the odds ratios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These diagnostic graphs allows you to look at the impact of the priors, see the parameter correlations, and inspect the probability density for the nuisance parameters (phi and noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next steps is to extract commonly used results from these probabilities, mainly:\n",
    "* The best fit parameters (MAP, MODE, etc) and an overplot with the observations\n",
    "* The confidence regions for each parameters. \n",
    "\n",
    "(see LINK TO NOTEBOOK)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.15 ('py39')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0d105923ff2751b07541a5477823aa5d88058b2fa83b28e1bf4a447aeb52c9df"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
